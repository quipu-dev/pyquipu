import json
import logging
import re
from typing import List, Dict, Set, Tuple, Optional

from .git_db import GitDB
from .sqlite_db import DatabaseManager
from .git_object_storage import GitObjectHistoryReader  # Reuse parsing logic

logger = logging.getLogger(__name__)


class Hydrator:
    """
    è´Ÿè´£å°† Git å¯¹è±¡å†å²è®°å½•åŒæ­¥ï¼ˆè¡¥æ°´ï¼‰åˆ° SQLite æ•°æ®åº“ã€‚
    """

    def __init__(self, git_db: GitDB, db_manager: DatabaseManager):
        self.git_db = git_db
        self.db_manager = db_manager
        # å¤ç”¨ Reader ä¸­çš„äºŒè¿›åˆ¶è§£æé€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
        self._parser = GitObjectHistoryReader(git_db)

    def _get_owner_from_ref(self, ref_name: str, local_user_id: str) -> Optional[str]:
        """ä» Git ref è·¯å¾„ä¸­è§£æ owner_idã€‚"""
        remote_match = re.match(r"refs/quipu/remotes/[^/]+/([^/]+)/heads/.*", ref_name)
        if remote_match:
            return remote_match.group(1)
        if ref_name.startswith("refs/quipu/local/heads/"):
            return local_user_id
        return None

    def _get_commit_owners(self, local_user_id: str) -> Dict[str, str]:
        """æ„å»ºä¸€ä¸ªä» commit_hash åˆ° owner_id çš„æ˜ å°„ã€‚"""
        ref_tuples = self.git_db.get_all_ref_heads("refs/quipu/")
        commit_to_owner: Dict[str, str] = {}
        for commit_hash, ref_name in ref_tuples:
            if commit_hash in commit_to_owner:
                continue
            owner_id = self._get_owner_from_ref(ref_name, local_user_id)
            if owner_id:
                commit_to_owner[commit_hash] = owner_id
        return commit_to_owner

    def sync(self, local_user_id: str):
        """
        æ‰§è¡Œå¢é‡è¡¥æ°´æ“ä½œã€‚
        æ­¤å®ç°ç»è¿‡é‡æ„ï¼Œä»¥ç¡®ä¿åœ¨ä»é›¶é‡å»ºæ—¶èƒ½å¤Ÿå¤„ç†å®Œæ•´çš„å†å²å›¾è°±ã€‚
        """
        # --- é˜¶æ®µ 1: å‘ç° ---
        all_ref_heads = [t[0] for t in self.git_db.get_all_ref_heads("refs/quipu/")]
        if not all_ref_heads:
            logger.debug("âœ… Git ä¸­æœªå‘ç° Quipu å¼•ç”¨ï¼Œæ— éœ€è¡¥æ°´ã€‚")
            return

        # 1.1 è·å–æ‰€æœ‰ Quipu å†å²ä¸­çš„å®Œæ•´ commit æ—¥å¿—
        all_git_logs = self.git_db.log_ref(all_ref_heads)
        if not all_git_logs:
            logger.debug("âœ… Git ä¸­æœªå‘ç° Quipu å†å²ï¼Œæ— éœ€è¡¥æ°´ã€‚")
            return
        log_map = {entry["hash"]: entry for entry in all_git_logs}
        
        # 1.2 ç¡®å®š HEAD commit çš„æ‰€æœ‰è€…
        commit_owners = self._get_commit_owners(local_user_id)

        # 1.3 è®¡ç®—éœ€è¦æ’å…¥çš„èŠ‚ç‚¹ (æ‰€æœ‰å†å²èŠ‚ç‚¹ - å·²åœ¨æ•°æ®åº“ä¸­çš„èŠ‚ç‚¹)
        db_hashes = self.db_manager.get_all_node_hashes()
        missing_hashes = set(log_map.keys()) - db_hashes
        
        if not missing_hashes:
            logger.debug("âœ… æ•°æ®åº“ä¸ Git å†å²ä¸€è‡´ï¼Œæ— éœ€è¡¥æ°´ã€‚")
            return
            
        logger.info(f"å‘ç° {len(missing_hashes)} ä¸ªéœ€è¦è¡¥æ°´çš„èŠ‚ç‚¹ã€‚")

        # --- é˜¶æ®µ 2: æ‰¹é‡å‡†å¤‡æ•°æ® ---
        nodes_to_insert: List[Tuple] = []
        edges_to_insert: List[Tuple] = []

        tree_hashes = [log_map[h]["tree"] for h in missing_hashes if h in log_map]
        trees_content = self.git_db.batch_cat_file(tree_hashes)

        tree_to_meta_blob: Dict[str, str] = {}
        meta_blob_hashes: List[str] = []
        for tree_hash, content_bytes in trees_content.items():
            entries = self._parser._parse_tree_binary(content_bytes)
            if "metadata.json" in entries:
                blob_hash = entries["metadata.json"]
                tree_to_meta_blob[tree_hash] = blob_hash
                meta_blob_hashes.append(blob_hash)
        metas_content = self.git_db.batch_cat_file(meta_blob_hashes)

        for commit_hash in missing_hashes:
            log_entry = log_map[commit_hash]
            tree_hash = log_entry["tree"]
            owner_id = commit_owners.get(commit_hash, local_user_id)

            meta_blob_hash = tree_to_meta_blob.get(tree_hash)
            if not meta_blob_hash or meta_blob_hash not in metas_content:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° metadata.json å†…å®¹")
                continue

            output_tree = self._parser._parse_output_tree_from_body(log_entry["body"])
            if not output_tree:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° Output-Tree trailer")
                continue

            try:
                meta_bytes = metas_content[meta_blob_hash]
                meta_data = json.loads(meta_bytes)
                nodes_to_insert.append(
                    (
                        commit_hash, owner_id, output_tree,
                        meta_data.get("type", "unknown"),
                        float(meta_data.get("exec", {}).get("start") or log_entry["timestamp"]),
                        meta_data.get("summary", "No summary"),
                        meta_data.get("generator", {}).get("id"),
                        meta_bytes.decode("utf-8"), None
                    )
                )
                for p_hash in log_entry["parent"].split():
                    if p_hash in log_map:
                        edges_to_insert.append((commit_hash, p_hash))
            except (json.JSONDecodeError, KeyError) as e:
                logger.error(f"è§£æ {commit_hash[:7]} çš„å…ƒæ•°æ®å¤±è´¥: {e}")

        # --- é˜¶æ®µ 3: æ‰¹é‡å†™å…¥æ•°æ®åº“ ---
        if nodes_to_insert:
            self.db_manager.batch_insert_nodes(nodes_to_insert)
            logger.info(f"ğŸ’§ {len(nodes_to_insert)} ä¸ªèŠ‚ç‚¹å…ƒæ•°æ®å·²è¡¥æ°´ã€‚")
        if edges_to_insert:
            self.db_manager.batch_insert_edges(edges_to_insert)
            logger.info(f"ğŸ’§ {len(edges_to_insert)} æ¡è¾¹å…³ç³»å·²è¡¥æ°´ã€‚")