# chore: å®ç° SQLite æ•°æ®è¡¥æ°´æœºåˆ¶

## ç”¨æˆ·éœ€æ±‚
å½“å‰ç³»ç»Ÿå·²ç»å®ç°äº†å‘ SQLite çš„â€œåŒå†™â€ï¼Œä½†ç¼ºå°‘ä¸€ä¸ªå…³é”®æœºåˆ¶ï¼šæ— æ³•ä»ä¸€ä¸ªå·²æœ‰çš„ Git å†å²è®°å½•ä¸­å¡«å……ï¼ˆæˆ–â€œè¡¥æ°´â€ï¼‰SQLite æ•°æ®åº“ã€‚è¿™å¯¼è‡´æ—§é¡¹ç›®æ— æ³•è¿ç§»åˆ°æ–°å­˜å‚¨åç«¯ï¼Œä¹Ÿä½¿å¾—ä»è¿œç¨‹æ‹‰å–çš„æ–°å†å²æ— æ³•è¢« UI æŸ¥è¯¢åˆ°ï¼Œä»è€Œé˜»ç¢äº†åç»­è¯»å–è·¯å¾„çš„ä¼˜åŒ–ã€‚

## è¯„è®º
è¿™æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„æ­¥éª¤ï¼Œå®ƒæ˜¯æ¿€æ´» SQLite é«˜æ€§èƒ½è¯»å–è·¯å¾„çš„åŸºçŸ³ã€‚å®ç°ä¸€ä¸ªå¥å£®ã€é«˜æ•ˆçš„è¡¥æ°´æœºåˆ¶ï¼Œå¯ä»¥ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›ç®¡ç†æœ¬åœ°ç¼“å­˜çš„å‘½ä»¤è¡Œå·¥å…·ã€‚

## ç›®æ ‡
1.  åœ¨ `quipu-engine` ä¸­åˆ›å»ºä¸€ä¸ª `Hydrator` ç±»ï¼Œè´Ÿè´£å°† Git å†å²åŒæ­¥åˆ° SQLiteã€‚
2.  å®ç°é«˜æ•ˆçš„å·®å¼‚è®¡ç®—å’Œæ‰¹é‡æ•°æ®è¯»å†™ï¼Œä»¥ç¡®ä¿è¡¥æ°´è¿‡ç¨‹æ€§èƒ½ä¼˜è‰¯ã€‚
3.  åœ¨ `Engine` å¯åŠ¨æ—¶è‡ªåŠ¨è§¦å‘å¢é‡è¡¥æ°´ï¼Œä¿è¯æ•°æ®æ–°é²œåº¦ã€‚
4.  å‘ `quipu-cli` æ·»åŠ  `quipu cache` å‘½ä»¤ï¼Œä¸ºç”¨æˆ·æä¾›æ‰‹åŠ¨ç®¡ç†æ•°æ®åº“ï¼ˆå¦‚é‡å»ºï¼‰çš„èƒ½åŠ›ã€‚

## åŸºæœ¬åŸç†
è¡¥æ°´æœºåˆ¶éµå¾ªâ€œGit ä¸ºå”¯ä¸€äº‹å®æ¥æºâ€çš„åŸåˆ™ã€‚å®ƒé€šè¿‡æ¯”è¾ƒ Git ä¸­çš„å…¨é‡ Quipu Commits å’Œ SQLite ä¸­çš„è®°å½•ï¼Œæ‰¾å‡ºå·®å¼‚ï¼Œç„¶åé€šè¿‡ Git çš„æ‰¹é‡è¯»å–æ¥å£ (`cat-file --batch`) é«˜æ•ˆè·å–ç¼ºå¤±èŠ‚ç‚¹çš„æ•°æ®ï¼Œæœ€åæ‰¹é‡å†™å…¥ SQLiteã€‚è¿™ä¸ªè¿‡ç¨‹è¢«è®¾è®¡ä¸ºå¹‚ç­‰çš„ï¼Œå¯ä»¥å®‰å…¨åœ°åå¤è¿è¡Œã€‚è¡¥æ°´å†™å…¥çš„æ•°æ®å†…å®¹ (`plan.md`) å°†è¢«æ ‡è®°ä¸ºâ€œå†·æ•°æ®â€ (`NULL`)ï¼Œéµå¾ªæ‡’åŠ è½½ç­–ç•¥ã€‚

## æ ‡ç­¾
#intent/tooling #flow/ready #priority/high
#comp/engine #comp/cli #concept/state #concept/history #scope/core
#ai/instruct
#task/domain/storage #task/object/sqlite-backend #task/action/integration
#task/state/continue #task/status/active

---

## Script

### Acts 1: å¼•æ“å±‚ - åˆ›å»º Hydrator åŠå…¶ä¾èµ–

æˆ‘ä»¬é¦–å…ˆåœ¨ `quipu-engine` ä¸­æ„å»ºè¡¥æ°´å™¨çš„æ ¸å¿ƒé€»è¾‘ã€‚è¿™åŒ…æ‹¬ `Hydrator` ç±»æœ¬èº«ï¼Œä»¥åŠ `DatabaseManager` ä¸­ç”¨äºæ”¯æŒé«˜æ•ˆæŸ¥è¯¢å’Œæ‰¹é‡æ’å…¥çš„æ–°æ–¹æ³•ã€‚

~~~~~act
write_file packages/quipu-engine/src/quipu/core/hydrator.py
~~~~~
~~~~~python
import json
import logging
import re
from typing import List, Dict, Set, Tuple

from .git_db import GitDB
from .sqlite_db import DatabaseManager
from .git_object_storage import GitObjectHistoryReader  # Reuse parsing logic

logger = logging.getLogger(__name__)


class Hydrator:
    """
    è´Ÿè´£å°† Git å¯¹è±¡å†å²è®°å½•åŒæ­¥ï¼ˆè¡¥æ°´ï¼‰åˆ° SQLite æ•°æ®åº“ã€‚
    """

    def __init__(self, git_db: GitDB, db_manager: DatabaseManager):
        self.git_db = git_db
        self.db_manager = db_manager
        # å¤ç”¨ Reader ä¸­çš„äºŒè¿›åˆ¶è§£æé€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
        self._parser = GitObjectHistoryReader(git_db)

    def _get_missing_commit_hashes(self) -> Set[str]:
        """
        è®¡ç®—å­˜åœ¨äº Git ä¸­ä½†ç¼ºå¤±äº SQLite çš„ commit å“ˆå¸Œé›†åˆã€‚
        """
        logger.debug("æ­£åœ¨è®¡ç®—éœ€è¦è¡¥æ°´çš„ Commit...")
        all_git_heads = self.git_db.get_all_ref_heads("refs/quipu/")
        if not all_git_heads:
            return set()

        git_log_entries = self.git_db.log_ref(all_git_heads)
        git_hashes = {entry["hash"] for entry in git_log_entries}
        
        db_hashes = self.db_manager.get_all_node_hashes()
        
        missing_hashes = git_hashes - db_hashes
        logger.info(f"å‘ç° {len(missing_hashes)} ä¸ªéœ€è¦è¡¥æ°´çš„èŠ‚ç‚¹ã€‚")
        return missing_hashes

    def sync(self):
        """
        æ‰§è¡Œå¢é‡è¡¥æ°´æ“ä½œã€‚
        """
        missing_hashes = self._get_missing_commit_hashes()
        if not missing_hashes:
            logger.debug("âœ… æ•°æ®åº“ä¸ Git å†å²ä¸€è‡´ï¼Œæ— éœ€è¡¥æ°´ã€‚")
            return

        all_git_logs = self.git_db.log_ref(self.git_db.get_all_ref_heads("refs/quipu/"))
        log_map = {entry["hash"]: entry for entry in all_git_logs}

        # --- æ‰¹é‡å‡†å¤‡æ•°æ® ---
        nodes_to_insert: List[Tuple] = []
        edges_to_insert: List[Tuple] = []

        # 1. æ‰¹é‡è·å– Trees
        tree_hashes = [log_map[h]["tree"] for h in missing_hashes]
        trees_content = self.git_db.batch_cat_file(tree_hashes)

        # 2. è§£æ Trees, æ‰¹é‡è·å– Metas
        tree_to_meta_blob: Dict[str, str] = {}
        meta_blob_hashes: List[str] = []
        for tree_hash, content_bytes in trees_content.items():
            entries = self._parser._parse_tree_binary(content_bytes)
            if "metadata.json" in entries:
                blob_hash = entries["metadata.json"]
                tree_to_meta_blob[tree_hash] = blob_hash
                meta_blob_hashes.append(blob_hash)

        metas_content = self.git_db.batch_cat_file(meta_blob_hashes)

        # 3. æ„å»ºæ’å…¥æ•°æ®
        for commit_hash in missing_hashes:
            log_entry = log_map[commit_hash]
            tree_hash = log_entry["tree"]
            
            meta_blob_hash = tree_to_meta_blob.get(tree_hash)
            if not meta_blob_hash:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° metadata.json")
                continue

            meta_bytes = metas_content.get(meta_blob_hash)
            if not meta_bytes:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° metadata blob")
                continue
            
            output_tree = self._parser._parse_output_tree_from_body(log_entry["body"])
            if not output_tree:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° Output-Tree trailer")
                continue

            try:
                meta_data = json.loads(meta_bytes)
                nodes_to_insert.append((
                    commit_hash,
                    output_tree,
                    meta_data.get("type", "unknown"),
                    float(meta_data.get("exec", {}).get("start") or log_entry["timestamp"]),
                    meta_data.get("summary", "No summary"),
                    meta_data.get("generator", {}).get("id"),
                    meta_bytes.decode('utf-8'),
                    None  # plan_md_cache is NULL for cold data
                ))

                # å¤„ç†è¾¹å…³ç³»
                parent_hashes = log_entry["parent"].split()
                for p_hash in parent_hashes:
                    edges_to_insert.append((commit_hash, p_hash))
            except (json.JSONDecodeError, KeyError) as e:
                logger.error(f"è§£æ {commit_hash[:7]} çš„å…ƒæ•°æ®å¤±è´¥: {e}")

        # --- æ‰¹é‡å†™å…¥æ•°æ®åº“ ---
        if nodes_to_insert:
            self.db_manager.batch_insert_nodes(nodes_to_insert)
            logger.info(f"ğŸ’§ {len(nodes_to_insert)} ä¸ªèŠ‚ç‚¹å…ƒæ•°æ®å·²è¡¥æ°´ã€‚")
        if edges_to_insert:
            self.db_manager.batch_insert_edges(edges_to_insert)
            logger.info(f"ğŸ’§ {len(edges_to_insert)} æ¡è¾¹å…³ç³»å·²è¡¥æ°´ã€‚")

~~~~~
~~~~~act
replace packages/quipu-engine/src/quipu/core/sqlite_db.py
~~~~~
~~~~~python
import sqlite3
import logging
from pathlib import Path
from typing import Optional, Any

logger = logging.getLogger(__name__)


class DatabaseManager:
    """
    ç®¡ç† SQLite æ•°æ®åº“è¿æ¥å’Œ Schemaã€‚
    """

    def __init__(self, work_dir: Path):
        self.db_path = work_dir / ".quipu" / "history.sqlite"
        self.db_path.parent.mkdir(exist_ok=True)
        self._conn: Optional[sqlite3.Connection] = None

    def _get_conn(self) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚"""
        if self._conn is None:
            try:
                self._conn = sqlite3.connect(self.db_path, check_same_thread=False)
                self._conn.row_factory = sqlite3.Row
                # å¼€å¯å¤–é”®çº¦æŸ
                self._conn.execute("PRAGMA foreign_keys = ON;")
                logger.debug(f"ğŸ—ƒï¸  æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")
            except sqlite3.Error as e:
                logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                raise
        return self._conn

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥ã€‚"""
        if self._conn:
            self._conn.close()
            self._conn = None
            logger.debug("ğŸ—ƒï¸  æ•°æ®åº“è¿æ¥å·²å…³é—­ã€‚")

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œä½œä¸ºå…³é—­è¿æ¥çš„æœ€åä¸€é“é˜²çº¿ã€‚"""
        self.close()

    def init_schema(self):
        """
        åˆå§‹åŒ–æ•°æ®åº“ Schemaï¼Œå¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚
        ç¬¦åˆ QLDS v1.0 è§„èŒƒã€‚
        """
        conn = self._get_conn()
        try:
            with conn:
                # nodes è¡¨
                conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS nodes (
                        commit_hash TEXT(40) PRIMARY KEY,
                        output_tree TEXT(40) NOT NULL,
                        node_type TEXT NOT NULL,
                        timestamp REAL NOT NULL,
                        summary TEXT NOT NULL,
                        generator_id TEXT,
                        meta_json TEXT NOT NULL,
                        plan_md_cache TEXT
                    );
                    """
                )
                # ç´¢å¼•
                conn.execute("CREATE INDEX IF NOT EXISTS IDX_nodes_timestamp ON nodes(timestamp);")
                conn.execute("CREATE INDEX IF NOT EXISTS IDX_nodes_output_tree ON nodes(output_tree);")

                # edges è¡¨
                conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS edges (
                        child_hash TEXT(40) NOT NULL,
                        parent_hash TEXT(40) NOT NULL,
                        PRIMARY KEY (child_hash, parent_hash),
                        FOREIGN KEY (child_hash) REFERENCES nodes(commit_hash) ON DELETE CASCADE,
                        FOREIGN KEY (parent_hash) REFERENCES nodes(commit_hash) ON DELETE CASCADE
                    );
                    """
                )
                conn.execute("CREATE INDEX IF NOT EXISTS IDX_edges_parent ON edges(parent_hash);")

                # private_data è¡¨
                conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS private_data (
                        node_hash TEXT(40) PRIMARY KEY,
                        intent_md TEXT,
                        ai_context TEXT,
                        created_at REAL DEFAULT (strftime('%s', 'now')),
                        FOREIGN KEY (node_hash) REFERENCES nodes(commit_hash) ON DELETE CASCADE
                    );
                    """
                )
            logger.debug("âœ… æ•°æ®åº“ Schema å·²åˆå§‹åŒ–/éªŒè¯ã€‚")
        except sqlite3.Error as e:
            logger.error(f"âŒ åˆå§‹åŒ– Schema å¤±è´¥: {e}")
            raise

    def execute_write(self, sql: str, params: tuple = ()):
        """æ‰§è¡Œå†™æ“ä½œçš„é€šç”¨æ–¹æ³•ã€‚"""
        conn = self._get_conn()
        try:
            with conn:
                conn.execute(sql, params)
        except sqlite3.Error as e:
            logger.error(f"âŒ æ•°æ®åº“å†™å…¥å¤±è´¥: {e} | SQL: {sql}")
            raise
~~~~~
~~~~~python
import sqlite3
import logging
from pathlib import Path
from typing import Optional, Any, Set, List, Tuple

logger = logging.getLogger(__name__)


class DatabaseManager:
    """
    ç®¡ç† SQLite æ•°æ®åº“è¿æ¥å’Œ Schemaã€‚
    """

    def __init__(self, work_dir: Path):
        self.db_path = work_dir / ".quipu" / "history.sqlite"
        self.db_path.parent.mkdir(exist_ok=True)
        self._conn: Optional[sqlite3.Connection] = None

    def _get_conn(self) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚"""
        if self._conn is None:
            try:
                self._conn = sqlite3.connect(self.db_path, check_same_thread=False)
                self._conn.row_factory = sqlite3.Row
                # å¼€å¯å¤–é”®çº¦æŸ
                self._conn.execute("PRAGMA foreign_keys = ON;")
                logger.debug(f"ğŸ—ƒï¸  æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")
            except sqlite3.Error as e:
                logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                raise
        return self._conn

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥ã€‚"""
        if self._conn:
            self._conn.close()
            self._conn = None
            logger.debug("ğŸ—ƒï¸  æ•°æ®åº“è¿æ¥å·²å…³é—­ã€‚")

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œä½œä¸ºå…³é—­è¿æ¥çš„æœ€åä¸€é“é˜²çº¿ã€‚"""
        self.close()

    def init_schema(self):
        """
        åˆå§‹åŒ–æ•°æ®åº“ Schemaï¼Œå¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚
        ç¬¦åˆ QLDS v1.0 è§„èŒƒã€‚
        """
        conn = self._get_conn()
        try:
            with conn:
                # nodes è¡¨
                conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS nodes (
                        commit_hash TEXT(40) PRIMARY KEY,
                        output_tree TEXT(40) NOT NULL,
                        node_type TEXT NOT NULL,
                        timestamp REAL NOT NULL,
                        summary TEXT NOT NULL,
                        generator_id TEXT,
                        meta_json TEXT NOT NULL,
                        plan_md_cache TEXT
                    );
                    """
                )
                # ç´¢å¼•
                conn.execute("CREATE INDEX IF NOT EXISTS IDX_nodes_timestamp ON nodes(timestamp);")
                conn.execute("CREATE INDEX IF NOT EXISTS IDX_nodes_output_tree ON nodes(output_tree);")

                # edges è¡¨
                conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS edges (
                        child_hash TEXT(40) NOT NULL,
                        parent_hash TEXT(40) NOT NULL,
                        PRIMARY KEY (child_hash, parent_hash),
                        FOREIGN KEY (child_hash) REFERENCES nodes(commit_hash) ON DELETE CASCADE,
                        FOREIGN KEY (parent_hash) REFERENCES nodes(commit_hash) ON DELETE CASCADE
                    );
                    """
                )
                conn.execute("CREATE INDEX IF NOT EXISTS IDX_edges_parent ON edges(parent_hash);")

                # private_data è¡¨
                conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS private_data (
                        node_hash TEXT(40) PRIMARY KEY,
                        intent_md TEXT,
                        ai_context TEXT,
                        created_at REAL DEFAULT (strftime('%s', 'now')),
                        FOREIGN KEY (node_hash) REFERENCES nodes(commit_hash) ON DELETE CASCADE
                    );
                    """
                )
            logger.debug("âœ… æ•°æ®åº“ Schema å·²åˆå§‹åŒ–/éªŒè¯ã€‚")
        except sqlite3.Error as e:
            logger.error(f"âŒ åˆå§‹åŒ– Schema å¤±è´¥: {e}")
            raise

    def execute_write(self, sql: str, params: tuple = ()):
        """æ‰§è¡Œå†™æ“ä½œçš„é€šç”¨æ–¹æ³•ã€‚"""
        conn = self._get_conn()
        try:
            with conn:
                conn.execute(sql, params)
        except sqlite3.Error as e:
            logger.error(f"âŒ æ•°æ®åº“å†™å…¥å¤±è´¥: {e} | SQL: {sql}")
            raise

    def get_all_node_hashes(self) -> Set[str]:
        """è·å–æ•°æ®åº“ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„ commit_hashã€‚"""
        conn = self._get_conn()
        try:
            cursor = conn.execute("SELECT commit_hash FROM nodes;")
            return {row[0] for row in cursor.fetchall()}
        except sqlite3.Error as e:
            logger.error(f"âŒ æŸ¥è¯¢èŠ‚ç‚¹å“ˆå¸Œå¤±è´¥: {e}")
            return set()

    def batch_insert_nodes(self, nodes: List[Tuple]):
        """æ‰¹é‡æ’å…¥èŠ‚ç‚¹ã€‚"""
        conn = self._get_conn()
        sql = """
            INSERT OR IGNORE INTO nodes 
            (commit_hash, output_tree, node_type, timestamp, summary, generator_id, meta_json, plan_md_cache)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """
        try:
            with conn:
                conn.executemany(sql, nodes)
        except sqlite3.Error as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥èŠ‚ç‚¹å¤±è´¥: {e}")
            raise

    def batch_insert_edges(self, edges: List[Tuple]):
        """æ‰¹é‡æ’å…¥è¾¹ã€‚"""
        conn = self._get_conn()
        sql = "INSERT OR IGNORE INTO edges (child_hash, parent_hash) VALUES (?, ?)"
        try:
            with conn:
                conn.executemany(sql, edges)
        except sqlite3.Error as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥è¾¹å¤±è´¥: {e}")
            raise
~~~~~

### Acts 2: å¼•æ“é›†æˆä¸ CLI å‘½ä»¤

ç°åœ¨ï¼Œæˆ‘ä»¬å°† `Hydrator` é›†æˆåˆ° `Engine` çš„å¯åŠ¨æµç¨‹ä¸­ï¼Œå¹¶æ·»åŠ  `quipu cache` å­å‘½ä»¤ã€‚

~~~~~act
replace packages/quipu-engine/src/quipu/core/state_machine.py
~~~~~
~~~~~python
import logging
import re
from pathlib import Path
from typing import Dict, Optional, List, Tuple, Any
from datetime import datetime

from .git_db import GitDB
from .config import ConfigManager
from quipu.core.models import QuipuNode
from quipu.core.storage import HistoryReader, HistoryWriter

# å¯¼å…¥ç±»å‹ä»¥è¿›è¡Œç±»å‹æç¤º
try:
    from .sqlite_db import DatabaseManager
except ImportError:
    DatabaseManager = None

logger = logging.getLogger(__name__)


class Engine:
    """
    Axon çŠ¶æ€å¼•æ“ã€‚
    è´Ÿè´£åè°ƒ Git ç‰©ç†çŠ¶æ€å’Œ Axon é€»è¾‘å›¾è°±ã€‚
    """
~~~~~
~~~~~python
import logging
import re
from pathlib import Path
from typing import Dict, Optional, List, Tuple, Any
from datetime import datetime

from .git_db import GitDB
from .config import ConfigManager
from .hydrator import Hydrator
from quipu.core.models import QuipuNode
from quipu.core.storage import HistoryReader, HistoryWriter

# å¯¼å…¥ç±»å‹ä»¥è¿›è¡Œç±»å‹æç¤º
try:
    from .sqlite_db import DatabaseManager
except ImportError:
    DatabaseManager = None

logger = logging.getLogger(__name__)


class Engine:
    """
    Axon çŠ¶æ€å¼•æ“ã€‚
    è´Ÿè´£åè°ƒ Git ç‰©ç†çŠ¶æ€å’Œ Axon é€»è¾‘å›¾è°±ã€‚
    """
~~~~~
~~~~~act
replace packages/quipu-engine/src/quipu/core/state_machine.py
~~~~~
~~~~~python
    def align(self) -> str:
        all_nodes = self.reader.load_all_nodes()
        final_graph: Dict[str, QuipuNode] = {}
        for node in all_nodes:
            if node.output_tree not in final_graph or node.timestamp > final_graph[node.output_tree].timestamp:
                final_graph[node.output_tree] = node
        self.history_graph = final_graph
        if all_nodes:
            logger.info(f"ä»å­˜å‚¨ä¸­åŠ è½½äº† {len(all_nodes)} ä¸ªå†å²äº‹ä»¶ï¼Œå½¢æˆ {len(final_graph)} ä¸ªå”¯ä¸€çŠ¶æ€èŠ‚ç‚¹ã€‚")

        current_hash = self.git_db.get_tree_hash()
        EMPTY_TREE_HASH = "4b825dc642cb6eb9a060e54bf8d69288fbee4904"
~~~~~
~~~~~python
    def align(self) -> str:
        # å¦‚æœä½¿ç”¨ SQLiteï¼Œå…ˆè¿›è¡Œæ•°æ®è¡¥æ°´
        if self.db_manager:
            try:
                hydrator = Hydrator(self.git_db, self.db_manager)
                hydrator.sync()
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨æ•°æ®è¡¥æ°´å¤±è´¥: {e}", exc_info=True)

        all_nodes = self.reader.load_all_nodes()
        final_graph: Dict[str, QuipuNode] = {}
        for node in all_nodes:
            if node.output_tree not in final_graph or node.timestamp > final_graph[node.output_tree].timestamp:
                final_graph[node.output_tree] = node
        self.history_graph = final_graph
        if all_nodes:
            logger.info(f"ä»å­˜å‚¨ä¸­åŠ è½½äº† {len(all_nodes)} ä¸ªå†å²äº‹ä»¶ï¼Œå½¢æˆ {len(final_graph)} ä¸ªå”¯ä¸€çŠ¶æ€èŠ‚ç‚¹ã€‚")

        current_hash = self.git_db.get_tree_hash()
        EMPTY_TREE_HASH = "4b825dc642cb6eb9a060e54bf8d69288fbee4904"
~~~~~
~~~~~act
replace packages/quipu-cli/src/quipu/cli/main.py
~~~~~
~~~~~python
# æ³¨æ„ï¼šä¸è¦åœ¨æ¨¡å—çº§åˆ«ç›´æ¥è°ƒç”¨ setup_logging()ï¼Œ
# å¦åˆ™ä¼šå¯¼è‡´ CliRunner æµ‹è¯•ä¸­çš„ I/O æµè¿‡æ—©ç»‘å®š/å…³é—­é—®é¢˜ã€‚
logger = logging.getLogger(__name__)

app = typer.Typer(add_completion=False, name="quipu")


def _prompt_for_confirmation(message: str, default: bool = False) -> bool:
~~~~~
~~~~~python
# æ³¨æ„ï¼šä¸è¦åœ¨æ¨¡å—çº§åˆ«ç›´æ¥è°ƒç”¨ setup_logging()ï¼Œ
# å¦åˆ™ä¼šå¯¼è‡´ CliRunner æµ‹è¯•ä¸­çš„ I/O æµè¿‡æ—©ç»‘å®š/å…³é—­é—®é¢˜ã€‚
logger = logging.getLogger(__name__)

app = typer.Typer(add_completion=False, name="quipu")
cache_app = typer.Typer(name="cache", help="ç®¡ç†æœ¬åœ° SQLite ç¼“å­˜ã€‚")
app.add_typer(cache_app)


def _prompt_for_confirmation(message: str, default: bool = False) -> bool:
~~~~~
~~~~~act
replace packages/quipu-cli/src/quipu/cli/main.py
~~~~~
~~~~~python
    ctx.exit(result.exit_code)


if __name__ == "__main__":
    app()
~~~~~
~~~~~python
    ctx.exit(result.exit_code)


@cache_app.command("sync")
def cache_sync(
    ctx: typer.Context,
    work_dir: Annotated[
        Path,
        typer.Option(
            "--work-dir", "-w", help="æ“ä½œæ‰§è¡Œçš„æ ¹ç›®å½•ï¼ˆå·¥ä½œåŒºï¼‰", file_okay=False, dir_okay=True, resolve_path=True
        ),
    ] = DEFAULT_WORK_DIR,
):
    """
    å°† Git å†å²å¢é‡åŒæ­¥åˆ° SQLite ç¼“å­˜ã€‚
    æ­¤æ“ä½œåœ¨å¤§å¤šæ•°å‘½ä»¤æ‰§è¡Œæ—¶ä¼šè‡ªåŠ¨è§¦å‘ã€‚
    """
    setup_logging()
    typer.secho("ğŸ’§ æ­£åœ¨æ‰§è¡Œå¢é‡æ•°æ®è¡¥æ°´...", fg=typer.colors.BLUE, err=True)
    try:
        engine = create_engine(work_dir)
        # create_engine å†…éƒ¨çš„ align() å·²ç»è§¦å‘äº† sync()
        typer.secho("âœ… æ•°æ®åŒæ­¥å®Œæˆã€‚", fg=typer.colors.GREEN, err=True)
    except Exception as e:
        typer.secho(f"âŒ æ•°æ®åŒæ­¥å¤±è´¥: {e}", fg=typer.colors.RED, err=True)
        ctx.exit(1)


@cache_app.command("rebuild")
def cache_rebuild(
    ctx: typer.Context,
    force: Annotated[bool, typer.Option("--force", "-f", help="å¼ºåˆ¶æ‰§è¡Œï¼Œè·³è¿‡ç¡®è®¤æç¤ºã€‚")] = False,
    work_dir: Annotated[
        Path,
        typer.Option(
            "--work-dir", "-w", help="æ“ä½œæ‰§è¡Œçš„æ ¹ç›®å½•ï¼ˆå·¥ä½œåŒºï¼‰", file_okay=False, dir_okay=True, resolve_path=True
        ),
    ] = DEFAULT_WORK_DIR,
):
    """
    å¼ºåˆ¶å…¨é‡é‡å»º SQLite ç¼“å­˜ã€‚
    è¿™å°†åˆ é™¤ç°æœ‰çš„æ•°æ®åº“æ–‡ä»¶ï¼Œå¹¶ä» Git å†å²ä¸­å®Œå…¨é‡æ–°ç”Ÿæˆã€‚
    """
    setup_logging()
    
    db_path = work_dir.resolve() / ".quipu" / "history.sqlite"
    if not db_path.exists():
        typer.secho("ğŸ¤· æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ç›´æ¥åˆ›å»ºã€‚æ— éœ€é‡å»ºã€‚", fg=typer.colors.YELLOW, err=True)
        cache_sync(ctx, work_dir)
        return

    if not force:
        prompt = f"ğŸš¨ å³å°†åˆ é™¤å¹¶é‡å»ºæ•°æ®åº“ {db_path}ã€‚\næ‰€æœ‰ç§æœ‰æ•°æ®ï¼ˆå¦‚å¼€å‘è€…æ„å›¾ï¼‰å°†ä¸¢å¤±ï¼æ­¤æ“ä½œä¸å¯é€†ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ"
        if not _prompt_for_confirmation(prompt, default=False):
            typer.secho("\nğŸš« æ“ä½œå·²å–æ¶ˆã€‚", fg=typer.colors.YELLOW, err=True)
            raise typer.Abort()
    
    try:
        db_path.unlink()
        typer.secho("ğŸ—‘ï¸  æ—§æ•°æ®åº“å·²åˆ é™¤ã€‚", err=True)
    except Exception as e:
        typer.secho(f"âŒ åˆ é™¤æ—§æ•°æ®åº“å¤±è´¥: {e}", fg=typer.colors.RED, err=True)
        ctx.exit(1)

    # è°ƒç”¨ sync ä¼šè‡ªåŠ¨å®Œæˆé‡å»º
    cache_sync(ctx, work_dir)


if __name__ == "__main__":
    app()
~~~~~

### Acts 3: æµ‹è¯•

æœ€åï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ–°çš„è¡¥æ°´æœºåˆ¶æ·»åŠ æµ‹è¯•ï¼Œç¡®ä¿å…¶åœ¨å„ç§åœºæ™¯ä¸‹éƒ½èƒ½æ­£ç¡®å·¥ä½œã€‚

~~~~~act
write_file tests/test_hydration.py
~~~~~
~~~~~python
import pytest
import subprocess
from pathlib import Path

from quipu.core.git_db import GitDB
from quipu.core.sqlite_db import DatabaseManager
from quipu.core.hydrator import Hydrator
from quipu.core.git_object_storage import GitObjectHistoryWriter

@pytest.fixture
def hydrator_setup(tmp_path: Path):
    """
    åˆ›å»ºä¸€ä¸ªåŒ…å« Git ä»“åº“ã€DB ç®¡ç†å™¨å’Œ Hydrator å®ä¾‹çš„æµ‹è¯•ç¯å¢ƒã€‚
    """
    repo_path = tmp_path / "hydro_repo"
    repo_path.mkdir()
    subprocess.run(["git", "init"], cwd=repo_path, check=True, capture_output=True)
    subprocess.run(["git", "config", "user.email", "test@quipu.dev"], cwd=repo_path, check=True)
    subprocess.run(["git", "config", "user.name", "Quipu Test"], cwd=repo_path, check=True)

    git_db = GitDB(repo_path)
    db_manager = DatabaseManager(repo_path)
    db_manager.init_schema()
    
    writer = GitObjectHistoryWriter(git_db)
    hydrator = Hydrator(git_db, db_manager)

    return hydrator, writer, git_db, db_manager, repo_path

class TestHydration:
    def test_full_hydration_from_scratch(self, hydrator_setup):
        """æµ‹è¯•ä»ä¸€ä¸ªç©ºçš„æ•°æ®åº“å¼€å§‹ï¼Œå®Œæ•´è¡¥æ°´ä¸€ä¸ªå·²æœ‰çš„ Git å†å²ã€‚"""
        hydrator, writer, git_db, db_manager, repo = hydrator_setup

        # 1. åœ¨ Git ä¸­åˆ›å»ºä¸¤ä¸ªèŠ‚ç‚¹
        (repo / "a.txt").touch()
        hash_a = git_db.get_tree_hash()
        writer.create_node("plan", "genesis", hash_a, "Node A")
        
        (repo / "b.txt").touch()
        hash_b = git_db.get_tree_hash()
        writer.create_node("plan", hash_a, hash_b, "Node B")

        # 2. åˆå§‹çŠ¶æ€ä¸‹ DB ä¸ºç©º
        assert len(db_manager.get_all_node_hashes()) == 0

        # 3. æ‰§è¡Œè¡¥æ°´
        hydrator.sync()

        # 4. éªŒè¯
        db_hashes = db_manager.get_all_node_hashes()
        assert len(db_hashes) == 2
        
        conn = db_manager._get_conn()
        # éªŒè¯ Node B çš„å†…å®¹
        node_b_row = conn.execute("SELECT * FROM nodes WHERE summary = ?", ("Node B",)).fetchone()
        assert node_b_row is not None
        assert node_b_row["plan_md_cache"] is None  # å¿…é¡»æ˜¯å†·æ•°æ®

        # éªŒè¯è¾¹å…³ç³»
        edge_row = conn.execute("SELECT * FROM edges WHERE child_hash = ?", (node_b_row["commit_hash"],)).fetchone()
        assert edge_row is not None

    def test_incremental_hydration(self, hydrator_setup):
        """æµ‹è¯•åªè¡¥æ°´å¢é‡éƒ¨åˆ†ã€‚"""
        hydrator, writer, git_db, db_manager, repo = hydrator_setup

        # 1. åˆ›å»ºèŠ‚ç‚¹ A å¹¶ç«‹å³è¡¥æ°´
        (repo / "a.txt").touch()
        hash_a = git_db.get_tree_hash()
        writer.create_node("plan", "genesis", hash_a, "Node A")
        hydrator.sync()
        assert len(db_manager.get_all_node_hashes()) == 1

        # 2. åˆ›å»ºèŠ‚ç‚¹ B
        (repo / "b.txt").touch()
        hash_b = git_db.get_tree_hash()
        writer.create_node("plan", hash_a, hash_b, "Node B")
        
        # 3. å†æ¬¡è¡¥æ°´
        hydrator.sync()

        # 4. éªŒè¯ï¼Œæ€»æ•°åº”ä¸º 2
        assert len(db_manager.get_all_node_hashes()) == 2
        
        conn = db_manager._get_conn()
        node_b_row = conn.execute("SELECT * FROM nodes WHERE summary = ?", ("Node B",)).fetchone()
        assert node_b_row is not None

    def test_hydration_idempotency(self, hydrator_setup):
        """æµ‹è¯•é‡å¤è¿è¡Œè¡¥æ°´ä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨ã€‚"""
        hydrator, writer, git_db, db_manager, repo = hydrator_setup
        
        (repo / "a.txt").touch()
        hash_a = git_db.get_tree_hash()
        writer.create_node("plan", "genesis", hash_a, "Node A")

        # è¿è¡Œä¸¤æ¬¡
        hydrator.sync()
        hydrator.sync()

        assert len(db_manager.get_all_node_hashes()) == 1
~~~~~
